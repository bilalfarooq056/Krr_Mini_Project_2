{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e4b3f9f",
   "metadata": {},
   "source": [
    "## Phase 1 NER Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d64237c",
   "metadata": {},
   "source": [
    "Make RDF's and then ceate classes object properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73176bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- cultural differences in pakistan --------\n",
    "\n",
    "\n",
    "# save as ner_crawl_spacy.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import spacy\n",
    "from urllib.parse import urlparse\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Choose model: en_core_web_sm (fast) or en_core_web_trf (better, heavy)\n",
    "MODEL_NAME = \"en_core_web_sm\"\n",
    "nlp = spacy.load(MODEL_NAME)\n",
    "\n",
    "# List of source URLs (the curated list above)\n",
    "URLS = [\n",
    "    \"https://scientiamag.org/the-cultural-diversity-of-pakistan/\",\n",
    "    # (2)\n",
    "    \"https://ideas.repec.org/a/rss/jnljms/v4i6p3.html\",\n",
    "    # (3)\n",
    "    \"https://www.scirp.org/journal/paperinformation?paperid=114000\",\n",
    "    # (4)\n",
    "    \"https://www.jstor.org/stable/2645700\",\n",
    "    # (5)\n",
    "    \"https://www.dawn.com/news/1865983\",\n",
    "    # (6)\n",
    "    \"https://www.richtmann.org/journal/index.php/mjss/article/download/10805/10421/41480\",\n",
    "    # (7)\n",
    "    \"https://www.researchgate.net/publication/374433555_Impact_of_Cultural_Norms_and_Social_Expectations_for_Shaping_Gender_Disparities_in_Educational_Attainment_in_Pakistan\",\n",
    "    # (8)\n",
    "    \"https://www.britannica.com/place/Pakistan/People\",\n",
    "    # (9)\n",
    "    \"https://www.researchgate.net/publication/260259174_Cultural_Diversity_in_Pakistan_National_vs_Provincial\",\n",
    "    # (10)\n",
    "    \"https://bmcwomenshealth.biomedcentral.com/articles/10.1186/s12905-022-02011-6\",\n",
    "    # (11)\n",
    "    \"https://www.researchgate.net/publication/249973777_Language_and_Ethnicity_in_Pakistan\",\n",
    "    # (12)\n",
    "    \"https://www.jsshuok.com/oj/index.php/jssh/article/download/181/154\",\n",
    "    # (13)\n",
    "    \"https://www.richtmann.org/journal/index.php/mjss/article/view/10805/10421\",\n",
    "    # (14)\n",
    "    \"https://www.socialsciencejournals.pjgs-ws.com/index.php/PJGS/article/view/752\",\n",
    "    # (15)\n",
    "    \"https://www.rjelal.com/5.4.17a/282-297%20JAVED%20IQBAL%20BERKI.pdf\",\n",
    "    # (16)\n",
    "    \"https://moderndiplomacy.eu/2023/03/11/diversity-in-pakistan-a-strength-or-weakness/\",\n",
    "    # (17)\n",
    "    \"https://pmc.ncbi.nlm.nih.gov/articles/PMC3208374/\",\n",
    "    # (18)\n",
    "    \"https://en.wikipedia.org/wiki/Languages_of_Pakistan\",\n",
    "    # (19)\n",
    "    \"https://assajournal.com/index.php/36/article/view/303\",\n",
    "    # (20)\n",
    "    \"https://pjhc.nihcr.edu.pk/wp-content/uploads/2023/02/1-Farhan-Siddiqi.pdf\",\n",
    "    # (21)\n",
    "    \"https://pjsel.jehanf.com/index.php/journal/article/view/1082\",\n",
    "    # (22)\n",
    "    \"https://remittancesreview.com/menu-script/index.php/remittances/article/download/2617/2141/6198\",\n",
    "    # (23) Reuters\n",
    "    \"https://www.reuters.com/world/asia-pacific/wow-how-driving-school-programme-empowers-pakistani-women-2024-12-30/\",\n",
    "    # (24) Guardian\n",
    "    \"https://www.theguardian.com/artanddesign/2024/dec/30/our-art-is-a-mirror-of-truth-pakistan-manzar-art-architecture-exhibition-national-museum-quatar\",\n",
    "    # (25) AP\n",
    "    \"https://apnews.com/article/956613fb40cf6fd5c1f329b5a12befc8\",\n",
    "    # (26)\n",
    "    \"https://www.london.ac.uk/news-events/student-blog/celebrating-culture-pakistan\",\n",
    "    # (27)\n",
    "    \"https://en.wikipedia.org/wiki/Marriage_in_Pakistan\",\n",
    "    # (28)\n",
    "    \"https://www.marjjan.com/blogs/fashion/traditional-pakistani-dresses-exploring-regional-styles-and-designs\",\n",
    "    # (29)\n",
    "    \"https://www.tastepak.com/p/culinary-diversity-unveiled-a-journey-through-the-regional-cuisines-of-pakistan\",\n",
    "    # (30)\n",
    "    \"https://travelpakistani.com/blogs/most-popular-cultural-festivals-in-pakistan/361\",\n",
    "    # (31)\n",
    "    \"https://www.globetrottingbooklovers.com/blog/a-wedding-in-pakistan\",\n",
    "    # (32)\n",
    "    \"https://en.wikipedia.org/wiki/Pakistani_clothing\",\n",
    "    # (33)\n",
    "    \"https://www.researchgate.net/publication/395381499_World_Cuisine_Pakistani_Cuisine_-_Street_Food\",\n",
    "    # (34)\n",
    "    \"https://www.researchgate.net/publication/392692831_ETHNIC_VS_WESTERN_ATTIRE_IN_THE_PAKISTANI_WORKPLACE_A_STUDY_OF_IDENTITY_CULTURE_AND_PROFESSIONALISM\",\n",
    "    # (35)\n",
    "    \"https://blog.google/around-the-globe/google-asia/explore-pakistans-diverse-culinary-heritage/\",\n",
    "    # (36)\n",
    "    \"https://apnews.com/article/17303312be7993d7fd6cc597e8a3a008\",\n",
    "    # (37)\n",
    "    \"https://apnews.com/article/bbebc541b403875a15c675bab353821c\",\n",
    "    # (38)\n",
    "    \"https://www.theguardian.com/world/2025/jan/27/stories-woven-in-cloth-in-pakistans-first-textile-museum\"\n",
    "]\n",
    "\n",
    "# Which entity labels to keep/save (spaCy labels)\n",
    "KEEP_LABELS = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"MONEY\",\"NORP\",\"LANGUAGE\"}\n",
    "\n",
    "def fetch_text_from_url(url, timeout=12):\n",
    "    \"\"\"Fetch visible text from a URL. Return (text, content_type). Skips PDFs for now.\"\"\"\n",
    "    try:\n",
    "        head = requests.head(url, allow_redirects=True, timeout=8)\n",
    "        ctype = head.headers.get(\"content-type\",\"\")\n",
    "        if \"pdf\" in ctype.lower() or url.lower().endswith(\".pdf\"):\n",
    "            return None, \"pdf\"\n",
    "    except Exception:\n",
    "        # fallback to GET if HEAD fails\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=12, headers={\"User-Agent\":\"Mozilla/5.0 (compatible; NER-bot/1.0)\"})\n",
    "        if resp.status_code != 200:\n",
    "            return None, f\"error:{resp.status_code}\"\n",
    "        ctype = resp.headers.get(\"content-type\",\"\")\n",
    "        if \"pdf\" in ctype.lower():\n",
    "            return None, \"pdf\"\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        # remove scripts/styles and nav/footer potential noise\n",
    "        for s in soup([\"script\",\"style\",\"header\",\"footer\",\"nav\",\"aside\",\"form\",\"noscript\"]):\n",
    "            s.extract()\n",
    "        texts = []\n",
    "        # Prefer article/body tags\n",
    "        article = soup.find(\"article\")\n",
    "        if article:\n",
    "            texts.append(article.get_text(separator=\" \", strip=True))\n",
    "        body = soup.find(\"body\")\n",
    "        if body:\n",
    "            texts.append(body.get_text(separator=\" \", strip=True))\n",
    "        full = \" \".join(t for t in texts if t)\n",
    "        # shorten repeating whitespace\n",
    "        full = re.sub(r\"\\s+\", \" \", full).strip()\n",
    "        return full[:500000], ctype\n",
    "    except Exception as e:\n",
    "        return None, f\"exception:{e}\"\n",
    "\n",
    "def extract_entities(text, url):\n",
    "    doc = nlp(text)\n",
    "    rows = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in KEEP_LABELS:\n",
    "            # capture sentence for context\n",
    "            sent = ent.sent.text.strip() if ent.sent else \"\"\n",
    "            rows.append({\n",
    "                \"source_url\": url,\n",
    "                \"entity_text\": ent.text,\n",
    "                \"label\": ent.label_,\n",
    "                \"start_char\": ent.start_char,\n",
    "                \"end_char\": ent.end_char,\n",
    "                \"context_sentence\": sent\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "# Main loop\n",
    "all_rows = []\n",
    "errors = []\n",
    "for url in tqdm(URLS, desc=\"Processing URLs\"):\n",
    "    text, ctype = fetch_text_from_url(url)\n",
    "    if text is None:\n",
    "        errors.append((url, ctype))\n",
    "        time.sleep(0.5)\n",
    "        continue\n",
    "    # Optional: chunk long text into paragraphs to avoid spaCy memory spikes\n",
    "    paragraphs = [p.strip() for p in re.split(r\"\\n{1,}|\\.\\s{2,}\", text) if len(p.strip())>30]\n",
    "    for para in paragraphs:\n",
    "        try:\n",
    "            rows = extract_entities(para, url)\n",
    "            all_rows.extend(rows)\n",
    "        except Exception as e:\n",
    "            errors.append((url, f\"ner_error:{e}\"))\n",
    "    time.sleep(0.8)  # polite delay\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(all_rows)\n",
    "if df.shape[0]==0:\n",
    "    print(\"No entities extracted. Check model or pages.\")\n",
    "else:\n",
    "    df = df[[\"source_url\",\"entity_text\",\"label\",\"start_char\",\"end_char\",\"context_sentence\"]]\n",
    "    df.to_csv(\"entities.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved {len(df)} entities to entities.csv\")\n",
    "\n",
    "# Save errors for review\n",
    "pd.DataFrame(errors, columns=[\"url\",\"issue\"]).to_csv(\"fetch_errors.csv\", index=False)\n",
    "print(f\"Fetch errors saved to fetch_errors.csv (count={len(errors)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb29c619",
   "metadata": {},
   "source": [
    "> ### Make RDF Triples using CSV file created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f033dcb",
   "metadata": {},
   "source": [
    ">What I did (Phase 1)\n",
    "\n",
    "- Loaded your uploaded file: /mnt/data/entities.csv.\n",
    "- Detected columns: source_url, entity_text, label, start_char, end_char, context_sentence.\n",
    "\n",
    "- Created an entities CSV (if needed) at: /mnt/data/entities_extracted.csv.\n",
    "\n",
    "- Built a triples CSV (subject, predicate, object) at: /mnt/data/triples.csv.\n",
    "\n",
    "- Wrote an RDF Turtle file at: /mnt/data/triples.ttl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ad712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Pipeline:\n",
    "1) Read CSV (either raw texts or pre-extracted entities).\n",
    "2) Run spaCy NER if `text` column exists -> save entities_extracted.csv\n",
    "3) Build triples CSV and a Turtle RDF file (triples.csv, triples.ttl)\n",
    "\"\"\"\n",
    "import os, re, csv, pandas as pd\n",
    "# save as ner_crawl_spacy.py\n",
    "import re\n",
    "import csv\n",
    "import spacy\n",
    "\n",
    "INPUT_PATH = \"entities.csv\"            # change as needed\n",
    "OUT_ENTITIES = \"entities_extracted.csv\"\n",
    "OUT_TRIPLES_CSV = \"triples.csv\"\n",
    "OUT_RDF = \"triples.ttl\"\n",
    "\n",
    "def make_uri(s):\n",
    "    slug = re.sub(r'[^a-zA-Z0-9_]', '_', s.strip())[:120]\n",
    "    return f\"http://example.org/resource/{slug}\"\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "print(\"Loaded\", INPUT_PATH, \"columns:\", list(df.columns))\n",
    "\n",
    "# If there's a text column, attempt spaCy extraction\n",
    "entities_df = None\n",
    "if 'text' in df.columns:\n",
    "    try:\n",
    "        import spacy\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        rows = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = str(row['text'])\n",
    "            doc = nlp(text)\n",
    "            for ent in doc.ents:\n",
    "                rows.append({\"source_row\": idx, \"text\": text,\n",
    "                             \"entity_text\": ent.text, \"entity_label\": ent.label_})\n",
    "        entities_df = pd.DataFrame(rows)\n",
    "    except Exception as e:\n",
    "        print(\"spaCy not available or failed:\", e)\n",
    "\n",
    "# If spaCy not run, try to infer entity columns\n",
    "if entities_df is None:\n",
    "    if {'entity_text','label'}.issubset(df.columns):\n",
    "        entities_df = df.rename(columns={'label':'entity_label'})[['entity_text','entity_label']].copy()\n",
    "    else:\n",
    "        # fallback: first column as entity_text\n",
    "        entities_df = pd.DataFrame({\n",
    "            'entity_text': df.iloc[:,0].astype(str),\n",
    "            'entity_label': ['UNKNOWN'] * len(df)\n",
    "        })\n",
    "\n",
    "entities_df.to_csv(OUT_ENTITIES, index=False)\n",
    "print(\"Entities saved to\", OUT_ENTITIES)\n",
    "\n",
    "# Build simple triples\n",
    "triples = []\n",
    "for i, r in entities_df.reset_index().iterrows():\n",
    "    ent = str(r['entity_text']).strip()\n",
    "    label = str(r.get('entity_label','UNKNOWN')).strip()\n",
    "    subj = make_uri(ent + \"_\" + str(i))\n",
    "    triples.append((subj, \"rdf:type\", f\"http://example.org/ontology/{label}\"))\n",
    "    triples.append((subj, \"rdfs:label\", f\"\\\"{ent}\\\"\"))\n",
    "    triples.append((subj, \"ex:hasText\", f\"\\\"{ent}\\\"\"))\n",
    "    triples.append((subj, \"ex:canonicalURI\", make_uri(ent)))\n",
    "\n",
    "# save triples CSV\n",
    "with open(OUT_TRIPLES_CSV, 'w', newline='', encoding='utf-8') as fh:\n",
    "    writer = csv.writer(fh)\n",
    "    writer.writerow(['subject','predicate','object'])\n",
    "    for s,p,o in triples:\n",
    "        writer.writerow([s,p,o])\n",
    "print(\"Triples CSV saved to\", OUT_TRIPLES_CSV)\n",
    "\n",
    "# write Turtle (basic)\n",
    "with open(OUT_RDF, 'w', encoding='utf-8') as f:\n",
    "    f.write('@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\\n')\n",
    "    f.write('@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\\n')\n",
    "    f.write('@prefix ex: <http://example.org/ontology/> .\\n\\n')\n",
    "    for s,p,o in triples:\n",
    "        subj = f\"<{s}>\"\n",
    "        if p == \"rdf:type\":\n",
    "            pred = \"rdf:type\"\n",
    "            obj = f\"<{o}>\"\n",
    "        elif p == \"rdfs:label\":\n",
    "            pred = \"rdfs:label\"\n",
    "            obj = o  # already quoted\n",
    "        elif p == \"ex:hasText\":\n",
    "            pred = \"ex:hasText\"\n",
    "            obj = o\n",
    "        elif p == \"ex:canonicalURI\":\n",
    "            pred = \"ex:canonicalURI\"\n",
    "            obj = f\"<{o}>\"\n",
    "        else:\n",
    "            pred = p\n",
    "            obj = f\"<{o}>\"\n",
    "        f.write(f\"{subj} {pred} {obj} .\\n\")\n",
    "print(\"Turtle saved to\", OUT_RDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e05e7f2",
   "metadata": {},
   "source": [
    "fix ttl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b489598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: triples_fixed.ttl\n",
      "Original backup: triples.ttl.bak\n"
     ]
    }
   ],
   "source": [
    "# fix_ttl.py\n",
    "import re, sys, shutil, io\n",
    "IN = \"triples.ttl\"\n",
    "OUT = \"triples_fixed.ttl\"\n",
    "\n",
    "# backup\n",
    "shutil.copyfile(IN, IN + \".bak\")\n",
    "\n",
    "with open(IN, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Remove python bytes repr like b'...'\n",
    "text = re.sub(r\"\\\\bb'([^']*)'\", r\"\\1\", text)\n",
    "text = re.sub(r'\\\\bb\"([^\"]*)\"', r'\\1', text)\n",
    "# patterns like b'<http://...>' or b\"<http://...>\" -> <http://...>\n",
    "text = re.sub(r\"b'(<[^']*>)'\", r\"\\1\", text)\n",
    "text = re.sub(r'b\"(<[^\"]*>)\"', r'\\1', text)\n",
    "# remove stray leading b' before quotes or angle-bracketed tokens\n",
    "text = text.replace(\"b'<\", \"<\").replace('b\"<', \"<\").replace(\">'\", \">\").replace('>\"', '>')\n",
    "\n",
    "# ensure prefixes exist\n",
    "prefixes = [\n",
    "\"@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\",\n",
    "\"@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\",\n",
    "\"@prefix ex:   <http://example.org/ontology/> .\",\n",
    "\"\"\n",
    "]\n",
    "for p in prefixes:\n",
    "    if p not in text:\n",
    "        # add all prefixes to top if any missing\n",
    "        text = \"\\n\".join(prefixes) + text\n",
    "        break\n",
    "\n",
    "with open(OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "print(\"Wrote:\", OUT)\n",
    "print(\"Original backup:\", IN + \".bak\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "080459b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in lines 5001 to 10000 : at line 1 of <>:\n",
      "Bad syntax (Prefix \"rdfs:\" not bound) at ^ in:\n",
      "\"b'<http://example.org/resource/Torkham_1249> '^b'rdfs:label \"Torkham\" .\\n<http://example.org/resource/Torkham_'...\"\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "g = Graph()\n",
    "with open(\"triples.ttl\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for start in range(0, len(lines), 5000):\n",
    "    chunk = \"\".join(lines[start:start+5000])\n",
    "    try:\n",
    "        g.parse(data=chunk, format=\"turtle\")\n",
    "    except Exception as e:\n",
    "        print(\"Error in lines\", start+1, \"to\", start+5000, \":\", e)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06e9aaa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadSyntax",
     "evalue": "at line 34040 of <>:\nBad syntax (expected '.' or '}' or ']' at end of statement) at ^ in:\n\"...b'n_Paro_8506> rdf:type <http://example.org/ontology/PERSON> .'^b'\\n<http://example.org/resource/Satin_Paro_8506> rdfs:label \"S'...\"",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3553\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[30], line 3\u001b[0m\n    g.parse(\"triples_fixed.ttl\", format=\"turtle\")\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/rdflib/graph.py:1656\u001b[0m in \u001b[1;35mparse\u001b[0m\n    raise se\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/rdflib/graph.py:1647\u001b[0m in \u001b[1;35mparse\u001b[0m\n    parser.parse(source, self, **args)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:2034\u001b[0m in \u001b[1;35mparse\u001b[0m\n    p.loadStream(stream)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:492\u001b[0m in \u001b[1;35mloadStream\u001b[0m\n    return self.loadBuf(stream.read())  # Not ideal\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:498\u001b[0m in \u001b[1;35mloadBuf\u001b[0m\n    self.feed(buf)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:524\u001b[0m in \u001b[1;35mfeed\u001b[0m\n    i = self.directiveOrStatement(s, j)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:545\u001b[0m in \u001b[1;35mdirectiveOrStatement\u001b[0m\n    return self.checkDot(argstr, j)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:1227\u001b[0m in \u001b[1;35mcheckDot\u001b[0m\n    self.BadSyntax(argstr, j, \"expected '.' or '}' or ']' at end of statement\")\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/rdflib/plugins/parsers/notation3.py:1743\u001b[0;36m in \u001b[0;35mBadSyntax\u001b[0;36m\n\u001b[0;31m    raise BadSyntax(self._thisDoc, self.lines, argstr, i, msg)\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>\u001b[0;36m\u001b[0m\n\u001b[0;31mBadSyntax\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from rdflib import Graph\n",
    "g = Graph()\n",
    "g.parse(\"triples_fixed.ttl\", format=\"turtle\")\n",
    "print(\"Triples loaded:\", len(g))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50967e",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758aa720",
   "metadata": {},
   "source": [
    "## Phase 2 Ontology Design\n",
    "( Use of protege software to create classes and entitites )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5cecfa",
   "metadata": {},
   "source": [
    "> we need to create classes bsaed on 'label' in entities.csv dataset,\n",
    "> here are the steps i followed:\n",
    "<pre style=\"background-color: #464745ff; padding: 10px; border-left: 5px solid #b47124ff;\">\n",
    "\n",
    "\n",
    "1. Go to  --> [ Entity ] Create classes and subClasses \n",
    "- Person\n",
    "- Community\n",
    "- owl:Thing\n",
    "- Currency\n",
    "- Date\n",
    "- Language\n",
    "- Orgnization\n",
    "- Location\n",
    "- Region  \n",
    "2. Go to  --> [ Object Properties ]\n",
    "- belongsTo\n",
    "- foundedBy\n",
    "- hasMember\n",
    "- celebrates\n",
    "- contains\n",
    "- hasValue\n",
    "- locatedIN\n",
    "- speaksLanguage\n",
    "- usedLn\n",
    "3. Go To  --> [ Data Properties ]\n",
    "- hasEstablishedYear\n",
    "- hasAlternateName\n",
    "- hasLanguageCode\n",
    "- hasRegionCode\n",
    "- hasPopularityScore\n",
    "- hasValue\n",
    "- hasDate\n",
    "- hasDescription\n",
    "- hasContextSentence\n",
    "- hasSourceURL\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf668e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b763ed4",
   "metadata": {},
   "source": [
    "## Phase 3 Knowledge Graph ( Appache Jena  )\n",
    "Turn entities.csv into RDF triples and load them into Jena Fuseki."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5980eaf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a1c4af",
   "metadata": {},
   "source": [
    "## Phase 4 â€“ Python Query Interface ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
