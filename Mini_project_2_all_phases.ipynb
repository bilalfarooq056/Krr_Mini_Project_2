{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92109ead",
   "metadata": {},
   "source": [
    "### mini_Project_2 group partner names:\n",
    "- Bilal Farooq\n",
    "- Saad Khan\n",
    "- Washam Bin Adnan\n",
    "- Alam Zeb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b3f9f",
   "metadata": {},
   "source": [
    "## Phase 1 NER Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d64237c",
   "metadata": {},
   "source": [
    "Make RDF's and then ceate classes object properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38081c4",
   "metadata": {},
   "source": [
    "> Phase 1: NER Extraction (3-4 days)\n",
    "1. text source (research abstracts/news articles) will be given per group\n",
    "2. Use spaCy's pre-trained NER to extract:\n",
    "3. Example: Persons, Organizations, And Locations, Additional: Dates, Money, etc.\n",
    "4. Save extracted entities to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73176bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- cultural differences in pakistan --------\n",
    "\n",
    "\n",
    "# save as ner_crawl_spacy.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import spacy\n",
    "from urllib.parse import urlparse\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Choose model: en_core_web_sm (fast) or en_core_web_trf (better, heavy)\n",
    "MODEL_NAME = \"en_core_web_sm\"\n",
    "nlp = spacy.load(MODEL_NAME)\n",
    "\n",
    "# List of source URLs (the curated list above)\n",
    "URLS = [\n",
    "    \"https://scientiamag.org/the-cultural-diversity-of-pakistan/\",\n",
    "    # (2)\n",
    "    \"https://ideas.repec.org/a/rss/jnljms/v4i6p3.html\",\n",
    "    # (3)\n",
    "    \"https://www.scirp.org/journal/paperinformation?paperid=114000\",\n",
    "    # (4)\n",
    "    \"https://www.jstor.org/stable/2645700\",\n",
    "    # (5)\n",
    "    \"https://www.dawn.com/news/1865983\",\n",
    "    # (6)\n",
    "    \"https://www.richtmann.org/journal/index.php/mjss/article/download/10805/10421/41480\",\n",
    "    # (7)\n",
    "    \"https://www.researchgate.net/publication/374433555_Impact_of_Cultural_Norms_and_Social_Expectations_for_Shaping_Gender_Disparities_in_Educational_Attainment_in_Pakistan\",\n",
    "    # (8)\n",
    "    \"https://www.britannica.com/place/Pakistan/People\",\n",
    "    # (9)\n",
    "    \"https://www.researchgate.net/publication/260259174_Cultural_Diversity_in_Pakistan_National_vs_Provincial\",\n",
    "    # (10)\n",
    "    \"https://bmcwomenshealth.biomedcentral.com/articles/10.1186/s12905-022-02011-6\",\n",
    "    # (11)\n",
    "    \"https://www.researchgate.net/publication/249973777_Language_and_Ethnicity_in_Pakistan\",\n",
    "    # (12)\n",
    "    \"https://www.jsshuok.com/oj/index.php/jssh/article/download/181/154\",\n",
    "    # (13)\n",
    "    \"https://www.richtmann.org/journal/index.php/mjss/article/view/10805/10421\",\n",
    "    # (14)\n",
    "    \"https://www.socialsciencejournals.pjgs-ws.com/index.php/PJGS/article/view/752\",\n",
    "    # (15)\n",
    "    \"https://www.rjelal.com/5.4.17a/282-297%20JAVED%20IQBAL%20BERKI.pdf\",\n",
    "    # (16)\n",
    "    \"https://moderndiplomacy.eu/2023/03/11/diversity-in-pakistan-a-strength-or-weakness/\",\n",
    "    # (17)\n",
    "    \"https://pmc.ncbi.nlm.nih.gov/articles/PMC3208374/\",\n",
    "    # (18)\n",
    "    \"https://en.wikipedia.org/wiki/Languages_of_Pakistan\",\n",
    "    # (19)\n",
    "    \"https://assajournal.com/index.php/36/article/view/303\",\n",
    "    # (20)\n",
    "    \"https://pjhc.nihcr.edu.pk/wp-content/uploads/2023/02/1-Farhan-Siddiqi.pdf\",\n",
    "    # (21)\n",
    "    \"https://pjsel.jehanf.com/index.php/journal/article/view/1082\",\n",
    "    # (22)\n",
    "    \"https://remittancesreview.com/menu-script/index.php/remittances/article/download/2617/2141/6198\",\n",
    "    # (23) Reuters\n",
    "    \"https://www.reuters.com/world/asia-pacific/wow-how-driving-school-programme-empowers-pakistani-women-2024-12-30/\",\n",
    "    # (24) Guardian\n",
    "    \"https://www.theguardian.com/artanddesign/2024/dec/30/our-art-is-a-mirror-of-truth-pakistan-manzar-art-architecture-exhibition-national-museum-quatar\",\n",
    "    # (25) AP\n",
    "    \"https://apnews.com/article/956613fb40cf6fd5c1f329b5a12befc8\",\n",
    "    # (26)\n",
    "    \"https://www.london.ac.uk/news-events/student-blog/celebrating-culture-pakistan\",\n",
    "    # (27)\n",
    "    \"https://en.wikipedia.org/wiki/Marriage_in_Pakistan\",\n",
    "    # (28)\n",
    "    \"https://www.marjjan.com/blogs/fashion/traditional-pakistani-dresses-exploring-regional-styles-and-designs\",\n",
    "    # (29)\n",
    "    \"https://www.tastepak.com/p/culinary-diversity-unveiled-a-journey-through-the-regional-cuisines-of-pakistan\",\n",
    "    # (30)\n",
    "    \"https://travelpakistani.com/blogs/most-popular-cultural-festivals-in-pakistan/361\",\n",
    "    # (31)\n",
    "    \"https://www.globetrottingbooklovers.com/blog/a-wedding-in-pakistan\",\n",
    "    # (32)\n",
    "    \"https://en.wikipedia.org/wiki/Pakistani_clothing\",\n",
    "    # (33)\n",
    "    \"https://www.researchgate.net/publication/395381499_World_Cuisine_Pakistani_Cuisine_-_Street_Food\",\n",
    "    # (34)\n",
    "    \"https://www.researchgate.net/publication/392692831_ETHNIC_VS_WESTERN_ATTIRE_IN_THE_PAKISTANI_WORKPLACE_A_STUDY_OF_IDENTITY_CULTURE_AND_PROFESSIONALISM\",\n",
    "    # (35)\n",
    "    \"https://blog.google/around-the-globe/google-asia/explore-pakistans-diverse-culinary-heritage/\",\n",
    "    # (36)\n",
    "    \"https://apnews.com/article/17303312be7993d7fd6cc597e8a3a008\",\n",
    "    # (37)\n",
    "    \"https://apnews.com/article/bbebc541b403875a15c675bab353821c\",\n",
    "    # (38)\n",
    "    \"https://www.theguardian.com/world/2025/jan/27/stories-woven-in-cloth-in-pakistans-first-textile-museum\"\n",
    "]\n",
    "\n",
    "# Which entity labels to keep/save (spaCy labels)\n",
    "KEEP_LABELS = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"MONEY\",\"NORP\",\"LANGUAGE\"}\n",
    "\n",
    "def fetch_text_from_url(url, timeout=12):\n",
    "    \"\"\"Fetch visible text from a URL. Return (text, content_type). Skips PDFs for now.\"\"\"\n",
    "    try:\n",
    "        head = requests.head(url, allow_redirects=True, timeout=8)\n",
    "        ctype = head.headers.get(\"content-type\",\"\")\n",
    "        if \"pdf\" in ctype.lower() or url.lower().endswith(\".pdf\"):\n",
    "            return None, \"pdf\"\n",
    "    except Exception:\n",
    "        # fallback to GET if HEAD fails\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=12, headers={\"User-Agent\":\"Mozilla/5.0 (compatible; NER-bot/1.0)\"})\n",
    "        if resp.status_code != 200:\n",
    "            return None, f\"error:{resp.status_code}\"\n",
    "        ctype = resp.headers.get(\"content-type\",\"\")\n",
    "        if \"pdf\" in ctype.lower():\n",
    "            return None, \"pdf\"\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        # remove scripts/styles and nav/footer potential noise\n",
    "        for s in soup([\"script\",\"style\",\"header\",\"footer\",\"nav\",\"aside\",\"form\",\"noscript\"]):\n",
    "            s.extract()\n",
    "        texts = []\n",
    "        # Prefer article/body tags\n",
    "        article = soup.find(\"article\")\n",
    "        if article:\n",
    "            texts.append(article.get_text(separator=\" \", strip=True))\n",
    "        body = soup.find(\"body\")\n",
    "        if body:\n",
    "            texts.append(body.get_text(separator=\" \", strip=True))\n",
    "        full = \" \".join(t for t in texts if t)\n",
    "        # shorten repeating whitespace\n",
    "        full = re.sub(r\"\\s+\", \" \", full).strip()\n",
    "        return full[:500000], ctype\n",
    "    except Exception as e:\n",
    "        return None, f\"exception:{e}\"\n",
    "\n",
    "def extract_entities(text, url):\n",
    "    doc = nlp(text)\n",
    "    rows = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in KEEP_LABELS:\n",
    "            # capture sentence for context\n",
    "            sent = ent.sent.text.strip() if ent.sent else \"\"\n",
    "            rows.append({\n",
    "                \"source_url\": url,\n",
    "                \"entity_text\": ent.text,\n",
    "                \"label\": ent.label_,\n",
    "                \"start_char\": ent.start_char,\n",
    "                \"end_char\": ent.end_char,\n",
    "                \"context_sentence\": sent\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "# Main loop\n",
    "all_rows = []\n",
    "errors = []\n",
    "for url in tqdm(URLS, desc=\"Processing URLs\"):\n",
    "    text, ctype = fetch_text_from_url(url)\n",
    "    if text is None:\n",
    "        errors.append((url, ctype))\n",
    "        time.sleep(0.5)\n",
    "        continue\n",
    "    # Optional: chunk long text into paragraphs to avoid spaCy memory spikes\n",
    "    paragraphs = [p.strip() for p in re.split(r\"\\n{1,}|\\.\\s{2,}\", text) if len(p.strip())>30]\n",
    "    for para in paragraphs:\n",
    "        try:\n",
    "            rows = extract_entities(para, url)\n",
    "            all_rows.extend(rows)\n",
    "        except Exception as e:\n",
    "            errors.append((url, f\"ner_error:{e}\"))\n",
    "    time.sleep(0.8)  # polite delay\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(all_rows)\n",
    "if df.shape[0]==0:\n",
    "    print(\"No entities extracted. Check model or pages.\")\n",
    "else:\n",
    "    df = df[[\"source_url\",\"entity_text\",\"label\",\"start_char\",\"end_char\",\"context_sentence\"]]\n",
    "    df.to_csv(\"entities.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved {len(df)} entities to entities.csv\")\n",
    "\n",
    "# Save errors for review\n",
    "pd.DataFrame(errors, columns=[\"url\",\"issue\"]).to_csv(\"fetch_errors.csv\", index=False)\n",
    "print(f\"Fetch errors saved to fetch_errors.csv (count={len(errors)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb29c619",
   "metadata": {},
   "source": [
    "> ### Make RDF Triples using CSV file created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ad712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Pipeline:\n",
    "1) Read CSV (either raw texts or pre-extracted entities).\n",
    "2) Run spaCy NER if `text` column exists -> save entities_extracted.csv\n",
    "3) Build triples CSV and a Turtle RDF file (triples.csv, triples.ttl)\n",
    "\"\"\"\n",
    "import os, re, csv, pandas as pd\n",
    "# save as ner_crawl_spacy.py\n",
    "import re\n",
    "import csv\n",
    "import spacy\n",
    "\n",
    "INPUT_PATH = \"entities.csv\"            # change as needed\n",
    "OUT_ENTITIES = \"entities_extracted.csv\"\n",
    "OUT_TRIPLES_CSV = \"triples.csv\"\n",
    "OUT_RDF = \"triples.ttl\"\n",
    "\n",
    "def make_uri(s):\n",
    "    slug = re.sub(r'[^a-zA-Z0-9_]', '_', s.strip())[:120]\n",
    "    return f\"http://example.org/resource/{slug}\"\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "print(\"Loaded\", INPUT_PATH, \"columns:\", list(df.columns))\n",
    "\n",
    "# If there's a text column, attempt spaCy extraction\n",
    "entities_df = None\n",
    "if 'text' in df.columns:\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        rows = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = str(row['text'])\n",
    "            doc = nlp(text)\n",
    "            for ent in doc.ents:\n",
    "                rows.append({\"source_row\": idx, \"text\": text,\n",
    "                             \"entity_text\": ent.text, \"entity_label\": ent.label_})\n",
    "        entities_df = pd.DataFrame(rows)\n",
    "    except Exception as e:\n",
    "        print(\"spaCy not available or failed:\", e)\n",
    "\n",
    "# If spaCy not run, try to infer entity columns\n",
    "if entities_df is None:\n",
    "    if {'entity_text','label'}.issubset(df.columns):\n",
    "        entities_df = df.rename(columns={'label':'entity_label'})[['entity_text','entity_label']].copy()\n",
    "    else:\n",
    "        # fallback: first column as entity_text\n",
    "        entities_df = pd.DataFrame({\n",
    "            'entity_text': df.iloc[:,0].astype(str),\n",
    "            'entity_label': ['UNKNOWN'] * len(df)\n",
    "        })\n",
    "\n",
    "entities_df.to_csv(OUT_ENTITIES, index=False)\n",
    "print(\"Entities saved to\", OUT_ENTITIES)\n",
    "\n",
    "# Build simple triples\n",
    "triples = []\n",
    "for i, r in entities_df.reset_index().iterrows():\n",
    "    ent = str(r['entity_text']).strip()\n",
    "    label = str(r.get('entity_label','UNKNOWN')).strip()\n",
    "    subj = make_uri(ent + \"_\" + str(i))\n",
    "    triples.append((subj, \"rdf:type\", f\"http://example.org/ontology/{label}\"))\n",
    "    triples.append((subj, \"rdfs:label\", f\"\\\"{ent}\\\"\"))\n",
    "    triples.append((subj, \"ex:hasText\", f\"\\\"{ent}\\\"\"))\n",
    "    triples.append((subj, \"ex:canonicalURI\", make_uri(ent)))\n",
    "\n",
    "# save triples CSV\n",
    "with open(OUT_TRIPLES_CSV, 'w', newline='', encoding='utf-8') as fh:\n",
    "    writer = csv.writer(fh)\n",
    "    writer.writerow(['subject','predicate','object'])\n",
    "    for s,p,o in triples:\n",
    "        writer.writerow([s,p,o])\n",
    "print(\"Triples CSV saved to\", OUT_TRIPLES_CSV)\n",
    "\n",
    "# write Turtle (basic)\n",
    "with open(OUT_RDF, 'w', encoding='utf-8') as f:\n",
    "    f.write('@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\\n')\n",
    "    f.write('@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\\n')\n",
    "    f.write('@prefix ex: <http://example.org/ontology/> .\\n\\n')\n",
    "    for s,p,o in triples:\n",
    "        subj = f\"<{s}>\"\n",
    "        if p == \"rdf:type\":\n",
    "            pred = \"rdf:type\"\n",
    "            obj = f\"<{o}>\"\n",
    "        elif p == \"rdfs:label\":\n",
    "            pred = \"rdfs:label\"\n",
    "            obj = o  # already quoted\n",
    "        elif p == \"ex:hasText\":\n",
    "            pred = \"ex:hasText\"\n",
    "            obj = o\n",
    "        elif p == \"ex:canonicalURI\":\n",
    "            pred = \"ex:canonicalURI\"\n",
    "            obj = f\"<{o}>\"\n",
    "        else:\n",
    "            pred = p\n",
    "            obj = f\"<{o}>\"\n",
    "        f.write(f\"{subj} {pred} {obj} .\\n\")\n",
    "print(\"Turtle saved to\", OUT_RDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5e492",
   "metadata": {},
   "source": [
    "### Convert .ttl to RDF  file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e9aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from rdflib import Graph\n",
    "g = Graph()\n",
    "g.parse(\"triples.ttl\", format=\"turtle\")\n",
    "print(\"Triples loaded:\", len(g))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af97b0",
   "metadata": {},
   "source": [
    ">What I did (Phase 1)\n",
    "\n",
    "- Loaded your uploaded file: /mnt/data/entities.csv.\n",
    "- Detected columns: source_url, entity_text, label, start_char, end_char, context_sentence.\n",
    "\n",
    "- Created an entities CSV (if needed) at: /mnt/data/entities_extracted.csv.\n",
    "\n",
    "- Built a triples CSV (subject, predicate, object) at: /mnt/data/triples.csv.\n",
    "\n",
    "- Add a properties file 'Cultural_difference_properties.ttl'\n",
    "\n",
    "- Wrote an RDF Turtle file at: /mnt/data/triples.ttl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50967e",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758aa720",
   "metadata": {},
   "source": [
    "## Phase 2 Ontology Design\n",
    "( Use of protege software to create classes and entitites )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15079317",
   "metadata": {},
   "source": [
    "> Phase 2: Basic Ontology (3-4 days)\n",
    "1. Design ontology with classes extracted from CSV e.g. if classes extracted are 2000 get 200 randomly\n",
    "or manually as it makes sense for your domain.\n",
    "2. Example: `Person`, `Organization`, `Location`, `Event`, `Document`\n",
    "3. Add object properties: `worksFor`, `locatedIn`, `hasAuthor` just an example\n",
    "4. Use any technique you like to map relations automatically rather than manually , be it machine\n",
    "learning model ,be it NLP model ,leverage existing resources as Wikidata or dbpedia\n",
    "5. Create in Protégé, export as RDF/OWL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5cecfa",
   "metadata": {},
   "source": [
    "> we need to create classes bsaed on 'label' in entities.csv dataset,\n",
    "> here are the steps i followed:\n",
    "<pre style=\"background-color: #464745ff; padding: 10px; border-left: 5px solid #b47124ff;\">\n",
    "\n",
    "- first extracted data from 38 different websites based on 'Topic Search', data collected was almost 13100, then this data used in NER Extraction\n",
    "three files created from this [ triples.ttl, triples.csv ], then later inserted a .ttl for ( Object properties and data properties )\n",
    "file named as 'cultural_difference_properties.ttl'\n",
    "\n",
    "1. Go to  --> [ Entity ] Create classes and subClasses \n",
    "2. Go to  --> [ Object Properties ]\n",
    "3. Go To  --> [ Data Properties ]\n",
    "4. Go To  --> [ Individuals ]\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf668e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b763ed4",
   "metadata": {},
   "source": [
    "## Phase 3 Knowledge Graph ( Appache Jena  )\n",
    "Turn entities.csv into RDF triples and load them into Jena Fuseki."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96604428",
   "metadata": {},
   "source": [
    "> Phase 3: Apache Jena Setup (2-3 days)\n",
    "1. Install Apache Jena Fuseki\n",
    "2. Load ontology into Jena TDB\n",
    "3. Set up SPARQL endpoint\n",
    "4. Test basic queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969720b1",
   "metadata": {},
   "source": [
    "Main reason for Uploading "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28231445",
   "metadata": {},
   "source": [
    "> * Queries:\n",
    "1. PREFIX : <http://example.org/culturaldifference#>\n",
    "SELECT ?s ?p ?o \n",
    "WHERE { ?s ?p ?o } \n",
    "LIMIT 50\n",
    "\n",
    "2. SELECT ?predicate (COUNT(?predicate) AS ?count)\n",
    "WHERE {\n",
    "  ?s ?predicate ?o .\n",
    "}\n",
    "GROUP BY ?predicate\n",
    "ORDER BY DESC(?count)\n",
    "LIMIT 10\n",
    "\n",
    "3. PREFIX : <http://example.org/culturaldifference#>\n",
    "SELECT ?class (COUNT(?instance) AS ?count)\n",
    "WHERE {\n",
    "  ?instance a ?class .\n",
    "}\n",
    "GROUP BY ?class\n",
    "ORDER BY DESC(?count)\n",
    "\n",
    "4. PREFIX : <http://example.org/ontology/>\n",
    "SELECT ?person\n",
    "WHERE {\n",
    "  ?person a :PERSON .\n",
    "}\n",
    "LIMIT 30\n",
    "\n",
    "5. PREFIX : <http://example.org/ontology/>\n",
    "SELECT ?org\n",
    "WHERE {\n",
    "  ?org a :ORG .\n",
    "}\n",
    "LIMIT 30\n",
    "\n",
    "6. PREFIX : <http://example.org/ontology/>\n",
    "SELECT ?lang\n",
    "WHERE {\n",
    "  ?lang a :LANGUAGE .\n",
    "}\n",
    "LIMIT 30\n",
    "\n",
    "7. PREFIX : <http://example.org/ontology/>\n",
    "SELECT ?loc\n",
    "WHERE {\n",
    "  ?loc a :LOC .\n",
    "}\n",
    "LIMIT 30\n",
    "\n",
    "8. PREFIX : <http://example.org/ontology/>\n",
    "SELECT ?gpe\n",
    "WHERE {\n",
    "  ?gpe a :GPE .\n",
    "}\n",
    "LIMIT 30\n",
    "\n",
    "9. PREFIX : <http://example.org/ontology/>\n",
    "SELECT ?entity ?class\n",
    "WHERE { ?entity a ?class . }\n",
    "LIMIT 20\n",
    "\n",
    "10. SELECT DISTINCT ?entity\n",
    "WHERE { ?entity ?p ?o . }\n",
    "LIMIT 50\n",
    "\n",
    "11. SELECT ?class (COUNT(?instance) AS ?count)\n",
    "WHERE {\n",
    "  ?instance a ?class .\n",
    "}\n",
    "GROUP BY ?class\n",
    "ORDER BY DESC(?count)\n",
    "\n",
    "12. PREFIX onto: <http://example.org/ontology/>\n",
    "\n",
    "SELECT ?person\n",
    "WHERE { ?person a onto:PERSON . }\n",
    "LIMIT 20\n",
    "\n",
    "\n",
    "13. PREFIX onto: <http://example.org/ontology/>\n",
    "\n",
    "SELECT ?org\n",
    "WHERE { ?org a onto:ORG . }\n",
    "LIMIT 30\n",
    "\n",
    "\n",
    "14. PREFIX onto: <http://example.org/ontology/>\n",
    "\n",
    "SELECT ?region\n",
    "WHERE { ?region a onto:GPE . }\n",
    "LIMIT 30\n",
    "\n",
    "\n",
    "15. PREFIX : <http://example.org/culturaldifference#>\n",
    "SELECT ?s ?p ?o WHERE { ?s ?p ?o } LIMIT 50\n",
    "\n",
    "16. PREFIX : <http://example.org/culturaldifference#>\n",
    "SELECT ?class (COUNT(?x) AS ?count)\n",
    "WHERE { ?x a ?class . }\n",
    "GROUP BY ?class\n",
    "ORDER BY DESC(?count)\n",
    "\n",
    "17. PREFIX : <http://example.org/culturaldifference#>\n",
    "SELECT DISTINCT ?entity\n",
    "WHERE { ?entity ?p ?o }\n",
    "LIMIT 50\n",
    "\n",
    "18. PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "PREFIX : <http://example.org/culturaldifference#>\n",
    "SELECT DISTINCT ?property\n",
    "WHERE { ?property a owl:ObjectProperty . }\n",
    "\n",
    "19. PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "PREFIX : <http://example.org/culturaldifference#>\n",
    "SELECT DISTINCT ?property\n",
    "WHERE { ?property a owl:DatatypeProperty . }\n",
    "\n",
    "20. PREFIX : <http://example.org/culturaldifference#>\n",
    "SELECT ?s ?p ?o\n",
    "WHERE {\n",
    "  ?s ?p ?o .\n",
    "  FILTER(STRSTARTS(STR(?s), STR(:)))\n",
    "}\n",
    "LIMIT 50\n",
    "\n",
    "21. PREFIX : <http://example.org/culturaldifference#>\n",
    "SELECT ?person ?org\n",
    "WHERE { ?person :worksFor ?org . }\n",
    "\n",
    "22. PREFIX : <http://example.org/culturaldifference#>\n",
    "SELECT ?group ?language\n",
    "WHERE { ?group :speaksLanguage ?language . }\n",
    "\n",
    "23. PREFIX : <http://example.org/culturaldifference#>\n",
    "SELECT ?festival ?region\n",
    "WHERE { ?festival :hasFestivalIn ?region . }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5980eaf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a1c4af",
   "metadata": {},
   "source": [
    "## Phase 4 – Python Query Interface ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a711463e",
   "metadata": {},
   "source": [
    "> Phase 4: Python Interface (3-4 days)\n",
    "1. Create Python script using `SPARQLWrapper`\n",
    "2. Implement functions: just an example\n",
    " - a. `add_entity(entity_type, entity_name)`\n",
    " - b. `query_entities(sparql_query)`\n",
    " - c. `find_relationships(entity)`\n",
    "3. Better use GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8871efbe",
   "metadata": {},
   "source": [
    "# In last phase we creatd UI using streamlit\n",
    "- Components/Steps required for this process:\n",
    "1. extract data\n",
    "2. save data as csv and .ttl\n",
    "3. then upload it into protege and make automation to create classes till individuals\n",
    "4. extract this data into .OWL/RDF file\n",
    "5. upload this file into Apache Jena ater installation\n",
    "6. Run muliple queries to retrieve required data\n",
    "7. use these queries and files to connect with streamlit app.\n",
    "8. lastly make sure before starting the app you uploaded data into apacheJena and connect with it then start streamlit app by\n",
    "> 9. python -m streamlit run app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d2d77f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
